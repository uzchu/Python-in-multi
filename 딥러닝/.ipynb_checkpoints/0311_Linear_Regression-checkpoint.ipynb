{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50.496375,  7.312312],\n",
       "       [69.017336,  9.94901 ],\n",
       "       [ 6.504676,  0.402668],\n",
       "       [47.202416,  7.372892],\n",
       "       [40.42739 ,  5.413481],\n",
       "       [15.258608,  1.689084],\n",
       "       [32.444642,  4.89831 ],\n",
       "       [51.332003,  7.443845],\n",
       "       [30.384774,  4.409411],\n",
       "       [11.333958,  2.317807],\n",
       "       [14.756302,  2.439615],\n",
       "       [67.13818 ,  9.791725],\n",
       "       [62.027474,  9.515288],\n",
       "       [30.262427,  4.384508],\n",
       "       [24.907328,  3.238266],\n",
       "       [49.818682,  6.639022],\n",
       "       [31.089285,  4.104561],\n",
       "       [37.437399,  4.86235 ],\n",
       "       [45.713274,  6.301314],\n",
       "       [20.135743,  3.134608],\n",
       "       [35.496204,  5.385869],\n",
       "       [35.217918,  4.491739],\n",
       "       [-2.462339,  0.328581],\n",
       "       [15.504408,  1.957047],\n",
       "       [ 8.540072,  0.986557],\n",
       "       [62.464219,  8.944165],\n",
       "       [29.169664,  4.514209],\n",
       "       [37.718168,  5.217733],\n",
       "       [14.635353,  1.504053],\n",
       "       [ 9.71068 ,  1.362839],\n",
       "       [55.952296,  7.431248],\n",
       "       [40.06951 ,  5.040947],\n",
       "       [68.572343,  9.664205],\n",
       "       [20.406405,  2.665425],\n",
       "       [24.264395,  4.173132],\n",
       "       [29.022853,  3.747696],\n",
       "       [38.145868,  5.721421],\n",
       "       [37.077037,  4.717864],\n",
       "       [66.600359,  9.692133],\n",
       "       [32.949568,  5.40078 ],\n",
       "       [31.220697,  3.920622],\n",
       "       [56.978772,  7.873336],\n",
       "       [54.274524,  7.230729],\n",
       "       [63.37192 ,  9.084045],\n",
       "       [15.688089,  1.812122],\n",
       "       [54.948229,  7.584506],\n",
       "       [29.672139,  3.722426],\n",
       "       [57.671203,  7.848726],\n",
       "       [25.526581,  3.111014],\n",
       "       [34.821091,  4.832707],\n",
       "       [ 3.690326,  1.141458],\n",
       "       [10.659872,  1.464306],\n",
       "       [16.703429,  2.743704],\n",
       "       [31.735639,  5.147993],\n",
       "       [14.917495,  2.669471],\n",
       "       [42.721395,  6.178294],\n",
       "       [27.291772,  4.260877],\n",
       "       [14.485518,  2.544589],\n",
       "       [21.443083,  3.22081 ],\n",
       "       [ 5.965769,  1.431867],\n",
       "       [33.247386,  4.574652],\n",
       "       [63.20145 ,  9.718899],\n",
       "       [46.459512,  6.699085],\n",
       "       [16.287332,  2.892028],\n",
       "       [19.58972 ,  2.772967],\n",
       "       [11.606064,  1.741008],\n",
       "       [49.159971,  7.176792],\n",
       "       [54.300311,  8.207979],\n",
       "       [10.494688,  0.834052],\n",
       "       [64.386625,  8.649873],\n",
       "       [57.418799,  8.726796],\n",
       "       [20.407553,  3.583534],\n",
       "       [37.676875,  4.827961],\n",
       "       [66.887593,  8.941677],\n",
       "       [67.211215,  9.083486],\n",
       "       [53.953845,  7.476288],\n",
       "       [58.742402,  8.192582],\n",
       "       [27.489283,  3.813982],\n",
       "       [52.332898,  7.323201],\n",
       "       [49.766573,  7.053972],\n",
       "       [21.009881,  3.478218],\n",
       "       [26.709189,  3.679299],\n",
       "       [18.990932,  2.946478],\n",
       "       [61.695823,  8.321406],\n",
       "       [-1.611522,  0.274487],\n",
       "       [-0.425445,  0.157369],\n",
       "       [31.036336,  4.785926],\n",
       "       [ 8.847062,  0.655377],\n",
       "       [42.408609,  5.746323],\n",
       "       [54.604421,  7.247305],\n",
       "       [51.321778,  7.5253  ],\n",
       "       [21.424367,  3.601442],\n",
       "       [53.946823,  7.233337],\n",
       "       [28.078049,  4.408537],\n",
       "       [65.410065,  8.773976],\n",
       "       [14.409394,  2.704944],\n",
       "       [14.189825,  1.684163],\n",
       "       [20.200516,  3.243969],\n",
       "       [14.223915,  2.663616],\n",
       "       [21.488734,  3.397118]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_regression = np.loadtxt('simple_regression.txt')\n",
    "x_data = simple_regression[:,1]\n",
    "y_data = simple_regression[:,0]\n",
    "\n",
    "simple_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as simple_regression:\n",
    "    X = tf.placeholder(tf.float32, [None], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None], name='Y')\n",
    "    lr = tf.constant(1e-3, tf.float32) #learning rate 1e-3:0.001\n",
    "    W = tf.get_variable('W', dtype=tf.float32, initializer=tf.constant(1.,tf.float32)) #상수는 랜덤하게 지정하지만, 여기선 1로 지정\n",
    "    b = tf.get_variable('b', dtype=tf.float32, initializer=tf.constant(1.,tf.float32))\n",
    "    \n",
    "    h = W*X+b #가설\n",
    "    cost = tf.reduce_mean(tf.square(tf.subtract(h,Y))) #가설의 최적 W값을 찾기 위한 함수 subtract:-, square: ^2\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost) #.minimize부분이 실제 트레이닝, 그전까진 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  1090.7887\n",
      "[8.367637634277344, 2.139159679412842]\n",
      "loss :  98.37295\n",
      "[6.266578674316406, 1.785127878189087]\n",
      "loss :  17.297186\n",
      "[6.871478080749512, 1.8580822944641113]\n",
      "loss :  10.653106\n",
      "[6.703018665313721, 1.809265375137329]\n",
      "loss :  10.088429\n",
      "[6.755533218383789, 1.7955007553100586]\n",
      "loss :  10.020667\n",
      "[6.744857311248779, 1.7719683647155762]\n",
      "loss :  9.993877\n",
      "[6.752201080322266, 1.7514749765396118]\n",
      "loss :  9.970813\n",
      "[6.754356384277344, 1.730358362197876]\n",
      "loss :  9.948431\n",
      "[6.757956504821777, 1.709662914276123]\n",
      "loss :  9.926472\n",
      "[6.761105537414551, 1.689087986946106]\n",
      "loss :  9.904909\n",
      "[6.764346122741699, 1.6687172651290894]\n",
      "loss :  9.883728\n",
      "[6.767522811889648, 1.6485247611999512]\n",
      "loss :  9.862932\n",
      "[6.770680904388428, 1.6285157203674316]\n",
      "loss :  9.842507\n",
      "[6.773807525634766, 1.6086864471435547]\n",
      "loss :  9.82245\n",
      "[6.776906967163086, 1.5890361070632935]\n",
      "loss :  9.802753\n",
      "[6.779978275299072, 1.5695627927780151]\n",
      "loss :  9.783409\n",
      "[6.783022403717041, 1.5502649545669556]\n",
      "loss :  9.764408\n",
      "[6.786037921905518, 1.5311408042907715]\n",
      "loss :  9.745755\n",
      "[6.789027690887451, 1.5121891498565674]\n",
      "loss :  9.727434\n",
      "[6.791989326477051, 1.493408203125]\n",
      "loss :  9.709438\n",
      "[6.794924736022949, 1.4747965335845947]\n",
      "loss :  9.691768\n",
      "[6.7978339195251465, 1.4563525915145874]\n",
      "loss :  9.674412\n",
      "[6.800716876983643, 1.4380748271942139]\n",
      "loss :  9.657368\n",
      "[6.803573131561279, 1.41996169090271]\n",
      "loss :  9.6406355\n",
      "[6.8064045906066895, 1.4020118713378906]\n",
      "loss :  9.624197\n",
      "[6.809209823608398, 1.3842236995697021]\n",
      "loss :  9.608057\n",
      "[6.811990261077881, 1.3665958642959595]\n",
      "loss :  9.592203\n",
      "[6.8147454261779785, 1.3491268157958984]\n",
      "loss :  9.576637\n",
      "[6.81747579574585, 1.3318151235580444]\n",
      "loss :  9.56135\n",
      "[6.820181846618652, 1.3146594762802124]\n",
      "loss :  9.546336\n",
      "[6.8228631019592285, 1.2976583242416382]\n",
      "loss :  9.531592\n",
      "[6.825520038604736, 1.2808104753494263]\n",
      "loss :  9.51711\n",
      "[6.828153610229492, 1.2641143798828125]\n",
      "loss :  9.50289\n",
      "[6.830763339996338, 1.2475687265396118]\n",
      "loss :  9.488923\n",
      "[6.833348751068115, 1.2311720848083496]\n",
      "loss :  9.475212\n",
      "[6.835912227630615, 1.21492338180542]\n",
      "loss :  9.4617405\n",
      "[6.838451385498047, 1.198820948600769]\n",
      "loss :  9.448513\n",
      "[6.840968132019043, 1.182863712310791]\n",
      "loss :  9.435524\n",
      "[6.8434624671936035, 1.1670502424240112]\n",
      "loss :  9.422769\n",
      "[6.84593391418457, 1.1513792276382446]\n",
      "loss :  9.410241\n",
      "[6.84838342666626, 1.1358494758605957]\n",
      "loss :  9.397937\n",
      "[6.850810527801514, 1.1204595565795898]\n",
      "loss :  9.385855\n",
      "[6.853216171264648, 1.105208396911621]\n",
      "loss :  9.373988\n",
      "[6.855599880218506, 1.0900945663452148]\n",
      "loss :  9.362337\n",
      "[6.857962131500244, 1.0751169919967651]\n",
      "loss :  9.350895\n",
      "[6.8603034019470215, 1.060274362564087]\n",
      "loss :  9.339653\n",
      "[6.8626227378845215, 1.0455653667449951]\n",
      "loss :  9.328615\n",
      "[6.864922046661377, 1.0309890508651733]\n",
      "loss :  9.317779\n",
      "[6.867199897766113, 1.016543984413147]\n",
      "loss :  9.307134\n",
      "[6.869458198547363, 1.0022292137145996]\n",
      "loss :  9.296683\n",
      "[6.871695518493652, 0.9880433082580566]\n",
      "loss :  9.286415\n",
      "[6.873912334442139, 0.9739851951599121]\n",
      "loss :  9.276336\n",
      "[6.876110076904297, 0.9600538611412048]\n",
      "loss :  9.266435\n",
      "[6.878287315368652, 0.9462479948997498]\n",
      "loss :  9.256711\n",
      "[6.8804450035095215, 0.9325665235519409]\n",
      "loss :  9.247159\n",
      "[6.8825836181640625, 0.9190083742141724]\n",
      "loss :  9.237783\n",
      "[6.884702205657959, 0.9055722951889038]\n",
      "loss :  9.228576\n",
      "[6.886802673339844, 0.8922574520111084]\n",
      "loss :  9.219531\n",
      "[6.888883590698242, 0.8790624737739563]\n",
      "loss :  9.210648\n",
      "[6.890946388244629, 0.8659864068031311]\n",
      "loss :  9.201929\n",
      "[6.892989635467529, 0.8530281186103821]\n",
      "loss :  9.193361\n",
      "[6.895015239715576, 0.8401867151260376]\n",
      "loss :  9.184949\n",
      "[6.897022247314453, 0.8274608850479126]\n",
      "loss :  9.176688\n",
      "[6.899011135101318, 0.8148497939109802]\n",
      "loss :  9.168575\n",
      "[6.90098237991333, 0.8023523688316345]\n",
      "loss :  9.160608\n",
      "[6.90293550491333, 0.7899674773216248]\n",
      "loss :  9.1527815\n",
      "[6.904871463775635, 0.7776942849159241]\n",
      "loss :  9.1451\n",
      "[6.906789779663086, 0.765531599521637]\n",
      "loss :  9.137552\n",
      "[6.908690452575684, 0.753478467464447]\n",
      "loss :  9.130141\n",
      "[6.910574436187744, 0.7415340542793274]\n",
      "loss :  9.122861\n",
      "[6.912441730499268, 0.7296972274780273]\n",
      "loss :  9.115715\n",
      "[6.9142913818359375, 0.7179669737815857]\n",
      "loss :  9.108692\n",
      "[6.916125297546387, 0.7063425183296204]\n",
      "loss :  9.101807\n",
      "[6.917942047119141, 0.6948227286338806]\n",
      "loss :  9.095032\n",
      "[6.919742107391357, 0.6834067106246948]\n",
      "loss :  9.088384\n",
      "[6.921526908874512, 0.6720936894416809]\n",
      "loss :  9.081859\n",
      "[6.923295021057129, 0.6608824729919434]\n",
      "loss :  9.075445\n",
      "[6.925047397613525, 0.6497723460197449]\n",
      "loss :  9.069146\n",
      "[6.926783561706543, 0.6387622952461243]\n",
      "loss :  9.062962\n",
      "[6.928504467010498, 0.6278514862060547]\n",
      "loss :  9.056892\n",
      "[6.930210113525391, 0.6170390248298645]\n",
      "loss :  9.050928\n",
      "[6.931899547576904, 0.6063238978385925]\n",
      "loss :  9.04507\n",
      "[6.933574676513672, 0.5957054495811462]\n",
      "loss :  9.039316\n",
      "[6.935234069824219, 0.5851825475692749]\n",
      "loss :  9.033669\n",
      "[6.9368791580200195, 0.5747545957565308]\n",
      "loss :  9.028123\n",
      "[6.938508987426758, 0.5644205212593079]\n",
      "loss :  9.0226755\n",
      "[6.940124034881592, 0.5541795492172241]\n",
      "loss :  9.017324\n",
      "[6.94172477722168, 0.5440308451652527]\n",
      "loss :  9.012072\n",
      "[6.9433112144470215, 0.5339736342430115]\n",
      "loss :  9.00691\n",
      "[6.944882869720459, 0.5240069627761841]\n",
      "loss :  9.001844\n",
      "[6.946440696716309, 0.514130175113678]\n",
      "loss :  8.996866\n",
      "[6.947984218597412, 0.5043423771858215]\n",
      "loss :  8.991981\n",
      "[6.949514389038086, 0.49464279413223267]\n",
      "loss :  8.987179\n",
      "[6.9510297775268555, 0.48503050208091736]\n",
      "loss :  8.982469\n",
      "[6.952532768249512, 0.47550496459007263]\n",
      "loss :  8.9778385\n",
      "[6.954021453857422, 0.4660651385784149]\n",
      "loss :  8.973293\n",
      "[6.955496788024902, 0.45671045780181885]\n",
      "loss :  8.968829\n",
      "[6.956958770751953, 0.4474400281906128]\n",
      "loss :  8.964444\n",
      "[6.958407878875732, 0.43825316429138184]\n",
      "loss :  8.96014\n",
      "[6.95984411239624, 0.4291490912437439]\n",
      "-----------------\n",
      "W = 6.95984, b = 0.42915\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=simple_regression) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        _, l = sess.run([train,cost],feed_dict={X:x_data, Y:y_data})\n",
    "        W_, b_ = sess.run([W,b])\n",
    "        print(\"loss : \", l)\n",
    "        print(f'[{W_}, {b_}]')\n",
    "    print('-----------------')\n",
    "    W_, b_ = sess.run([W,b])\n",
    "    print(f'W = {W_:.5f}, b = {b_:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.154169 1.6324354\n"
     ]
    }
   ],
   "source": [
    "plt.plot(x_data, x_data*W_ + b_, 'r')\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.show()\n",
    "print(W_, b_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_x, multi_y = [],[]\n",
    "multi_raw = np.loadtxt('multi_regression.txt')\n",
    "multi_y = multi_raw[:,2]\n",
    "multi_x = multi_raw[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as multi_regression:\n",
    "    num_x = 2\n",
    "    X = tf.placeholder(tf.float32,[None,num_x], name='X')\n",
    "    Y = tf.placeholder(tf.float32,[None], name='Y')\n",
    "    lr = tf.constant(1e-3, tf.float32)\n",
    "    W = tf.get_variable('W',[1,num_x],tf.float32)\n",
    "    b = tf.get_variable('b',dtype=tf.float32,initializer=tf.constant(1.,tf.float32))\n",
    "    \n",
    "    h = tf.matmul(W, X, transpose_b = True) + b\n",
    "    cost = tf.reduce_mean(tf.square(tf.subtract(h,Y)))\n",
    "    train = tf.train.GradientDescentOptimizer(lr).minimize(cost)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.29928267\n",
      "loss 0.29686415\n",
      "loss 0.29449543\n",
      "loss 0.29217532\n",
      "loss 0.28990278\n",
      "loss 0.28767672\n",
      "loss 0.28549597\n",
      "loss 0.28335962\n",
      "loss 0.2812665\n",
      "loss 0.27921578\n",
      "loss 0.27720636\n",
      "loss 0.27523732\n",
      "loss 0.27330774\n",
      "loss 0.27141672\n",
      "loss 0.2695634\n",
      "loss 0.26774687\n",
      "loss 0.26596645\n",
      "loss 0.26422104\n",
      "loss 0.2625102\n",
      "loss 0.26083285\n",
      "loss 0.2591883\n",
      "loss 0.25757596\n",
      "loss 0.255995\n",
      "loss 0.2544446\n",
      "loss 0.25292438\n",
      "loss 0.2514334\n",
      "loss 0.2499711\n",
      "loss 0.2485369\n",
      "loss 0.24713004\n",
      "loss 0.24575007\n",
      "loss 0.24439617\n",
      "loss 0.24306801\n",
      "loss 0.24176487\n",
      "loss 0.24048634\n",
      "loss 0.23923174\n",
      "loss 0.2380006\n",
      "loss 0.23679237\n",
      "loss 0.23560658\n",
      "loss 0.2344428\n",
      "loss 0.2333004\n",
      "loss 0.2321792\n",
      "loss 0.23107842\n",
      "loss 0.22999768\n",
      "loss 0.2289367\n",
      "loss 0.22789493\n",
      "loss 0.22687201\n",
      "loss 0.22586754\n",
      "loss 0.22488114\n",
      "loss 0.2239124\n",
      "loss 0.22296079\n",
      "loss 0.22202615\n",
      "loss 0.22110808\n",
      "loss 0.22020623\n",
      "loss 0.21932018\n",
      "loss 0.21844962\n",
      "loss 0.21759427\n",
      "loss 0.21675384\n",
      "loss 0.21592788\n",
      "loss 0.21511626\n",
      "loss 0.2143184\n",
      "loss 0.21353438\n",
      "loss 0.21276371\n",
      "loss 0.21200605\n",
      "loss 0.21126124\n",
      "loss 0.21052897\n",
      "loss 0.20980895\n",
      "loss 0.20910105\n",
      "loss 0.20840485\n",
      "loss 0.2077202\n",
      "loss 0.20704687\n",
      "loss 0.20638461\n",
      "loss 0.20573322\n",
      "loss 0.20509239\n",
      "loss 0.204462\n",
      "loss 0.20384184\n",
      "loss 0.2032315\n",
      "loss 0.20263109\n",
      "loss 0.20204023\n",
      "loss 0.20145872\n",
      "loss 0.20088644\n",
      "loss 0.20032313\n",
      "loss 0.1997687\n",
      "loss 0.19922289\n",
      "loss 0.19868554\n",
      "loss 0.19815652\n",
      "loss 0.19763558\n",
      "loss 0.19712259\n",
      "loss 0.19661751\n",
      "loss 0.19612\n",
      "loss 0.19563001\n",
      "loss 0.19514742\n",
      "loss 0.19467199\n",
      "loss 0.19420362\n",
      "loss 0.19374219\n",
      "loss 0.19328758\n",
      "loss 0.1928396\n",
      "loss 0.19239806\n",
      "loss 0.191963\n",
      "loss 0.1915342\n",
      "loss 0.19111153\n",
      "loss 0.19069485\n",
      "loss 0.19028416\n",
      "loss 0.18987915\n",
      "loss 0.18947989\n",
      "loss 0.18908615\n",
      "loss 0.18869796\n",
      "loss 0.1883151\n",
      "loss 0.18793738\n",
      "loss 0.18756488\n",
      "loss 0.1871974\n",
      "loss 0.18683489\n",
      "loss 0.18647729\n",
      "loss 0.1861244\n",
      "loss 0.1857762\n",
      "loss 0.18543252\n",
      "loss 0.18509336\n",
      "loss 0.18475862\n",
      "loss 0.18442813\n",
      "loss 0.18410195\n",
      "loss 0.18377993\n",
      "loss 0.183462\n",
      "loss 0.18314801\n",
      "loss 0.18283802\n",
      "loss 0.18253182\n",
      "loss 0.18222943\n",
      "loss 0.1819307\n",
      "loss 0.18163568\n",
      "loss 0.18134418\n",
      "loss 0.1810562\n",
      "loss 0.18077156\n",
      "loss 0.1804904\n",
      "loss 0.18021256\n",
      "loss 0.17993794\n",
      "loss 0.17966652\n",
      "loss 0.17939827\n",
      "loss 0.17913297\n",
      "loss 0.17887077\n",
      "loss 0.17861149\n",
      "loss 0.1783551\n",
      "loss 0.17810157\n",
      "loss 0.17785081\n",
      "loss 0.17760278\n",
      "loss 0.17735752\n",
      "loss 0.17711487\n",
      "loss 0.17687483\n",
      "loss 0.17663726\n",
      "loss 0.17640224\n",
      "loss 0.1761697\n",
      "loss 0.17593952\n",
      "loss 0.17571174\n",
      "loss 0.17548624\n",
      "loss 0.17526306\n",
      "loss 0.17504208\n",
      "loss 0.17482331\n",
      "loss 0.17460671\n",
      "loss 0.17439225\n",
      "loss 0.17417984\n",
      "loss 0.17396948\n",
      "loss 0.17376114\n",
      "loss 0.17355478\n",
      "loss 0.17335027\n",
      "loss 0.17314771\n",
      "loss 0.1729471\n",
      "loss 0.17274816\n",
      "loss 0.17255108\n",
      "loss 0.17235586\n",
      "loss 0.17216226\n",
      "loss 0.17197034\n",
      "loss 0.17178026\n",
      "loss 0.17159167\n",
      "loss 0.17140476\n",
      "loss 0.17121944\n",
      "loss 0.17103565\n",
      "loss 0.1708534\n",
      "loss 0.17067263\n",
      "loss 0.17049347\n",
      "loss 0.17031565\n",
      "loss 0.17013922\n",
      "loss 0.16996416\n",
      "loss 0.16979066\n",
      "loss 0.16961843\n",
      "loss 0.16944739\n",
      "loss 0.16927777\n",
      "loss 0.16910952\n",
      "loss 0.16894242\n",
      "loss 0.16877662\n",
      "loss 0.16861194\n",
      "loss 0.16844854\n",
      "loss 0.16828626\n",
      "loss 0.1681252\n",
      "loss 0.16796526\n",
      "loss 0.16780643\n",
      "loss 0.1676487\n",
      "loss 0.16749203\n",
      "loss 0.16733642\n",
      "loss 0.16718191\n",
      "loss 0.1670284\n",
      "loss 0.16687584\n",
      "loss 0.1667243\n",
      "loss 0.16657376\n",
      "loss 0.16642411\n",
      "loss 0.16627547\n",
      "loss 0.16612776\n",
      "loss 0.16598096\n",
      "loss 0.16583492\n",
      "loss 0.16568983\n",
      "loss 0.1655457\n",
      "loss 0.16540237\n",
      "loss 0.16525982\n",
      "loss 0.16511816\n",
      "loss 0.16497725\n",
      "loss 0.16483715\n",
      "loss 0.16469783\n",
      "loss 0.16455923\n",
      "loss 0.16442145\n",
      "loss 0.1642844\n",
      "loss 0.16414806\n",
      "loss 0.16401243\n",
      "loss 0.1638776\n",
      "loss 0.16374336\n",
      "loss 0.16360986\n",
      "loss 0.16347694\n",
      "loss 0.1633448\n",
      "loss 0.16321325\n",
      "loss 0.1630824\n",
      "loss 0.1629521\n",
      "loss 0.16282246\n",
      "loss 0.16269347\n",
      "loss 0.16256502\n",
      "loss 0.16243717\n",
      "loss 0.16230994\n",
      "loss 0.16218325\n",
      "loss 0.16205709\n",
      "loss 0.16193153\n",
      "loss 0.16180646\n",
      "loss 0.16168201\n",
      "loss 0.16155805\n",
      "loss 0.16143459\n",
      "loss 0.16131166\n",
      "loss 0.1611892\n",
      "loss 0.16106728\n",
      "loss 0.1609458\n",
      "loss 0.16082488\n",
      "loss 0.16070437\n",
      "loss 0.1605843\n",
      "loss 0.16046472\n",
      "loss 0.16034558\n",
      "loss 0.16022691\n",
      "loss 0.16010866\n",
      "loss 0.15999088\n",
      "loss 0.15987352\n",
      "loss 0.15975645\n",
      "loss 0.15963988\n",
      "loss 0.15952383\n",
      "loss 0.159408\n",
      "loss 0.15929264\n",
      "loss 0.15917769\n",
      "loss 0.15906304\n",
      "loss 0.15894881\n",
      "loss 0.15883496\n",
      "loss 0.15872148\n",
      "loss 0.15860829\n",
      "loss 0.15849555\n",
      "loss 0.15838316\n",
      "loss 0.15827106\n",
      "loss 0.15815926\n",
      "loss 0.15804788\n",
      "loss 0.15793674\n",
      "loss 0.15782598\n",
      "loss 0.15771547\n",
      "loss 0.15760536\n",
      "loss 0.1574955\n",
      "loss 0.15738598\n",
      "loss 0.15727682\n",
      "loss 0.15716787\n",
      "loss 0.15705922\n",
      "loss 0.15695086\n",
      "loss 0.1568428\n",
      "loss 0.15673499\n",
      "loss 0.15662746\n",
      "loss 0.15652025\n",
      "loss 0.15641335\n",
      "loss 0.15630654\n",
      "loss 0.15620014\n",
      "loss 0.15609396\n",
      "loss 0.15598804\n",
      "loss 0.15588228\n",
      "loss 0.15577689\n",
      "loss 0.15567167\n",
      "loss 0.15556672\n",
      "loss 0.15546206\n",
      "loss 0.15535751\n",
      "loss 0.15525323\n",
      "loss 0.15514925\n",
      "loss 0.15504543\n",
      "loss 0.15494187\n",
      "loss 0.1548385\n",
      "loss 0.15473542\n",
      "loss 0.15463242\n",
      "loss 0.15452972\n",
      "loss 0.1544272\n",
      "loss 0.15432495\n",
      "loss 0.1542228\n",
      "loss 0.15412092\n",
      "loss 0.15401922\n",
      "loss 0.15391773\n",
      "loss 0.15381637\n",
      "loss 0.15371524\n",
      "loss 0.15361431\n",
      "loss 0.15351355\n",
      "loss 0.15341303\n",
      "loss 0.15331256\n",
      "loss 0.15321234\n",
      "loss 0.15311228\n",
      "loss 0.15301242\n",
      "loss 0.15291274\n",
      "loss 0.15281323\n",
      "loss 0.15271388\n",
      "loss 0.15261474\n",
      "loss 0.15251574\n",
      "loss 0.15241691\n",
      "loss 0.15231818\n",
      "loss 0.15221958\n",
      "loss 0.15212122\n",
      "loss 0.15202305\n",
      "loss 0.15192497\n",
      "loss 0.15182708\n",
      "loss 0.15172927\n",
      "loss 0.15163165\n",
      "loss 0.15153417\n",
      "loss 0.1514369\n",
      "loss 0.15133974\n",
      "loss 0.15124263\n",
      "loss 0.15114573\n",
      "loss 0.15104897\n",
      "loss 0.15095237\n",
      "loss 0.15085585\n",
      "loss 0.15075952\n",
      "loss 0.15066329\n",
      "loss 0.15056719\n",
      "loss 0.15047128\n",
      "loss 0.15037546\n",
      "loss 0.15027972\n",
      "loss 0.15018418\n",
      "loss 0.15008874\n",
      "loss 0.14999336\n",
      "loss 0.1498982\n",
      "loss 0.14980313\n",
      "loss 0.14970818\n",
      "loss 0.14961332\n",
      "loss 0.14951858\n",
      "loss 0.149424\n",
      "loss 0.1493295\n",
      "loss 0.14923511\n",
      "loss 0.14914088\n",
      "loss 0.14904669\n",
      "loss 0.1489527\n",
      "loss 0.14885879\n",
      "loss 0.14876497\n",
      "loss 0.14867124\n",
      "loss 0.14857762\n",
      "loss 0.14848416\n",
      "loss 0.1483908\n",
      "loss 0.14829746\n",
      "loss 0.14820431\n",
      "loss 0.1481112\n",
      "loss 0.14801829\n",
      "loss 0.14792538\n",
      "loss 0.1478326\n",
      "loss 0.14773998\n",
      "loss 0.14764743\n",
      "loss 0.14755493\n",
      "loss 0.14746259\n",
      "loss 0.14737031\n",
      "loss 0.1472781\n",
      "loss 0.14718598\n",
      "loss 0.14709398\n",
      "loss 0.14700207\n",
      "loss 0.14691032\n",
      "loss 0.1468186\n",
      "loss 0.14672704\n",
      "loss 0.14663549\n",
      "loss 0.14654407\n",
      "loss 0.1464527\n",
      "loss 0.14636138\n",
      "loss 0.14627026\n",
      "loss 0.14617917\n",
      "loss 0.14608817\n",
      "loss 0.14599724\n",
      "loss 0.14590642\n",
      "loss 0.14581567\n",
      "loss 0.14572504\n",
      "loss 0.14563444\n",
      "loss 0.145544\n",
      "loss 0.14545356\n",
      "loss 0.1453633\n",
      "loss 0.145273\n",
      "loss 0.14518286\n",
      "loss 0.14509276\n",
      "loss 0.14500281\n",
      "loss 0.14491288\n",
      "loss 0.14482306\n",
      "loss 0.14473328\n",
      "loss 0.14464362\n",
      "loss 0.14455405\n",
      "loss 0.14446457\n",
      "loss 0.14437513\n",
      "loss 0.14428575\n",
      "loss 0.1441964\n",
      "loss 0.14410725\n",
      "loss 0.14401814\n",
      "loss 0.14392905\n",
      "loss 0.14384007\n",
      "loss 0.14375111\n",
      "loss 0.14366226\n",
      "loss 0.14357352\n",
      "loss 0.14348482\n",
      "loss 0.14339623\n",
      "loss 0.14330766\n",
      "loss 0.14321926\n",
      "loss 0.1431308\n",
      "loss 0.14304249\n",
      "loss 0.14295425\n",
      "loss 0.14286606\n",
      "loss 0.14277792\n",
      "loss 0.14268988\n",
      "loss 0.14260195\n",
      "loss 0.14251404\n",
      "loss 0.14242621\n",
      "loss 0.14233842\n",
      "loss 0.14225075\n",
      "loss 0.14216304\n",
      "loss 0.14207552\n",
      "loss 0.14198801\n",
      "loss 0.1419006\n",
      "loss 0.14181323\n",
      "loss 0.14172594\n",
      "loss 0.14163871\n",
      "loss 0.14155155\n",
      "loss 0.1414644\n",
      "loss 0.14137742\n",
      "loss 0.14129046\n",
      "loss 0.14120357\n",
      "loss 0.1411167\n",
      "loss 0.14102998\n",
      "loss 0.14094324\n",
      "loss 0.14085662\n",
      "loss 0.14076999\n",
      "loss 0.1406835\n",
      "loss 0.14059702\n",
      "loss 0.14051065\n",
      "loss 0.14042434\n",
      "loss 0.1403381\n",
      "loss 0.14025185\n",
      "loss 0.14016576\n",
      "loss 0.14007965\n",
      "loss 0.13999364\n",
      "loss 0.13990769\n",
      "loss 0.13982181\n",
      "loss 0.13973595\n",
      "loss 0.1396502\n",
      "loss 0.13956456\n",
      "loss 0.13947885\n",
      "loss 0.13939324\n",
      "loss 0.13930777\n",
      "loss 0.13922232\n",
      "loss 0.13913688\n",
      "loss 0.1390516\n",
      "loss 0.13896632\n",
      "loss 0.13888112\n",
      "loss 0.13879597\n",
      "loss 0.13871081\n",
      "loss 0.13862579\n",
      "loss 0.13854076\n",
      "loss 0.13845587\n",
      "loss 0.138371\n",
      "loss 0.1382862\n",
      "loss 0.13820143\n",
      "loss 0.13811672\n",
      "loss 0.13803212\n",
      "loss 0.13794747\n",
      "loss 0.13786301\n",
      "loss 0.13777849\n",
      "loss 0.13769415\n",
      "loss 0.1376098\n",
      "loss 0.13752551\n",
      "loss 0.13744128\n",
      "loss 0.13735709\n",
      "loss 0.13727297\n",
      "loss 0.13718894\n",
      "loss 0.13710496\n",
      "loss 0.13702093\n",
      "loss 0.13693708\n",
      "loss 0.13685319\n",
      "loss 0.13676946\n",
      "loss 0.13668573\n",
      "loss 0.13660209\n",
      "loss 0.13651851\n",
      "loss 0.13643491\n",
      "loss 0.13635139\n",
      "loss 0.13626795\n",
      "loss 0.13618457\n",
      "loss 0.13610123\n",
      "loss 0.13601792\n",
      "loss 0.13593474\n",
      "loss 0.13585155\n",
      "loss 0.13576846\n",
      "loss 0.13568538\n",
      "loss 0.1356024\n",
      "loss 0.13551943\n",
      "loss 0.13543655\n",
      "loss 0.13535365\n",
      "loss 0.13527088\n",
      "loss 0.13518818\n",
      "loss 0.13510552\n",
      "loss 0.13502285\n",
      "loss 0.13494027\n",
      "loss 0.1348577\n",
      "loss 0.13477531\n",
      "loss 0.13469286\n",
      "loss 0.13461046\n",
      "loss 0.13452823\n",
      "loss 0.134446\n",
      "loss 0.13436379\n",
      "loss 0.1342816\n",
      "loss 0.13419951\n",
      "loss 0.13411744\n",
      "loss 0.13403547\n",
      "loss 0.13395354\n",
      "loss 0.1338717\n",
      "loss 0.13378987\n",
      "loss 0.13370809\n",
      "loss 0.13362637\n",
      "loss 0.13354468\n",
      "loss 0.13346305\n",
      "loss 0.13338146\n",
      "loss 0.13329995\n",
      "loss 0.13321853\n",
      "loss 0.13313708\n",
      "loss 0.13305572\n",
      "loss 0.13297439\n",
      "loss 0.13289313\n",
      "loss 0.13281187\n",
      "loss 0.13273077\n",
      "loss 0.13264975\n",
      "loss 0.13256863\n",
      "loss 0.13248761\n",
      "loss 0.13240673\n",
      "loss 0.13232575\n",
      "loss 0.13224496\n",
      "loss 0.13216415\n",
      "loss 0.13208339\n",
      "loss 0.13200273\n",
      "loss 0.13192213\n",
      "loss 0.13184157\n",
      "loss 0.13176095\n",
      "loss 0.13168052\n",
      "loss 0.1316001\n",
      "loss 0.1315197\n",
      "loss 0.1314394\n",
      "loss 0.13135909\n",
      "loss 0.13127895\n",
      "loss 0.13119873\n",
      "loss 0.13111854\n",
      "loss 0.13103852\n",
      "loss 0.1309585\n",
      "loss 0.13087846\n",
      "loss 0.13079858\n",
      "loss 0.13071872\n",
      "loss 0.13063885\n",
      "loss 0.13055909\n",
      "loss 0.1304794\n",
      "loss 0.1303997\n",
      "loss 0.13032004\n",
      "loss 0.13024047\n",
      "loss 0.13016096\n",
      "loss 0.13008147\n",
      "loss 0.13000205\n",
      "loss 0.12992266\n",
      "loss 0.12984332\n",
      "loss 0.12976405\n",
      "loss 0.12968478\n",
      "loss 0.12960564\n",
      "loss 0.12952642\n",
      "loss 0.1294474\n",
      "loss 0.12936829\n",
      "loss 0.12928934\n",
      "loss 0.12921044\n",
      "loss 0.12913153\n",
      "loss 0.12905268\n",
      "loss 0.12897392\n",
      "loss 0.12889516\n",
      "loss 0.12881644\n",
      "loss 0.12873784\n",
      "loss 0.12865922\n",
      "loss 0.12858069\n",
      "loss 0.12850219\n",
      "loss 0.12842372\n",
      "loss 0.12834534\n",
      "loss 0.12826698\n",
      "loss 0.12818867\n",
      "loss 0.12811036\n",
      "loss 0.12803218\n",
      "loss 0.12795404\n",
      "loss 0.1278759\n",
      "loss 0.12779787\n",
      "loss 0.12771985\n",
      "loss 0.12764186\n",
      "loss 0.12756398\n",
      "loss 0.12748608\n",
      "loss 0.12740824\n",
      "loss 0.12733051\n",
      "loss 0.12725276\n",
      "loss 0.12717508\n",
      "loss 0.12709746\n",
      "loss 0.12701991\n",
      "loss 0.1269424\n",
      "loss 0.1268649\n",
      "loss 0.1267875\n",
      "loss 0.12671007\n",
      "loss 0.1266327\n",
      "loss 0.1265554\n",
      "loss 0.12647815\n",
      "loss 0.12640104\n",
      "loss 0.12632383\n",
      "loss 0.12624672\n",
      "loss 0.1261697\n",
      "loss 0.12609266\n",
      "loss 0.1260157\n",
      "loss 0.12593885\n",
      "loss 0.12586196\n",
      "loss 0.12578508\n",
      "loss 0.12570836\n",
      "loss 0.1256316\n",
      "loss 0.125555\n",
      "loss 0.12547834\n",
      "loss 0.1254017\n",
      "loss 0.1253252\n",
      "loss 0.1252487\n",
      "loss 0.12517226\n",
      "loss 0.1250959\n",
      "loss 0.12501952\n",
      "loss 0.12494328\n",
      "loss 0.12486698\n",
      "loss 0.124790765\n",
      "loss 0.12471459\n",
      "loss 0.124638535\n",
      "loss 0.12456246\n",
      "loss 0.12448645\n",
      "loss 0.124410465\n",
      "loss 0.12433454\n",
      "loss 0.124258675\n",
      "loss 0.124182776\n",
      "loss 0.124107\n",
      "loss 0.12403127\n",
      "loss 0.12395562\n",
      "loss 0.12387993\n",
      "loss 0.12380435\n",
      "loss 0.12372875\n",
      "loss 0.12365327\n",
      "loss 0.12357781\n",
      "loss 0.12350242\n",
      "loss 0.12342705\n",
      "loss 0.123351716\n",
      "loss 0.12327641\n",
      "loss 0.12320118\n",
      "loss 0.123125955\n",
      "loss 0.123050846\n",
      "loss 0.122975744\n",
      "loss 0.12290077\n",
      "loss 0.122825705\n",
      "loss 0.1227508\n",
      "loss 0.12267588\n",
      "loss 0.12260103\n",
      "loss 0.12252621\n",
      "loss 0.12245147\n",
      "loss 0.12237668\n",
      "loss 0.12230208\n",
      "loss 0.12222743\n",
      "loss 0.12215283\n",
      "loss 0.122078285\n",
      "loss 0.12200376\n",
      "loss 0.12192939\n",
      "loss 0.121854916\n",
      "loss 0.12178059\n",
      "loss 0.12170627\n",
      "loss 0.121632025\n",
      "loss 0.12155783\n",
      "loss 0.121483564\n",
      "loss 0.12140951\n",
      "loss 0.1213354\n",
      "loss 0.12126137\n",
      "loss 0.121187374\n",
      "loss 0.12111342\n",
      "loss 0.12103949\n",
      "loss 0.120965675\n",
      "loss 0.120891824\n",
      "loss 0.12081806\n",
      "loss 0.12074433\n",
      "loss 0.120670676\n",
      "loss 0.12059698\n",
      "loss 0.1205235\n",
      "loss 0.12044994\n",
      "loss 0.12037643\n",
      "loss 0.12030299\n",
      "loss 0.120229565\n",
      "loss 0.12015615\n",
      "loss 0.12008289\n",
      "loss 0.12000954\n",
      "loss 0.11993635\n",
      "loss 0.11986319\n",
      "loss 0.11979\n",
      "loss 0.119716965\n",
      "loss 0.11964389\n",
      "loss 0.119570866\n",
      "loss 0.119497895\n",
      "loss 0.11942501\n",
      "loss 0.119352125\n",
      "loss 0.11927934\n",
      "loss 0.11920656\n",
      "loss 0.119133785\n",
      "loss 0.119061075\n",
      "loss 0.11898849\n",
      "loss 0.1189159\n",
      "loss 0.118843295\n",
      "loss 0.11877076\n",
      "loss 0.118698284\n",
      "loss 0.11862588\n",
      "loss 0.118553504\n",
      "loss 0.1184812\n",
      "loss 0.11840887\n",
      "loss 0.11833662\n",
      "loss 0.11826439\n",
      "loss 0.11819228\n",
      "loss 0.11812017\n",
      "loss 0.11804809\n",
      "loss 0.11797601\n",
      "loss 0.11790407\n",
      "loss 0.11783217\n",
      "loss 0.11776018\n",
      "loss 0.11768837\n",
      "loss 0.11761652\n",
      "loss 0.117544815\n",
      "loss 0.11747309\n",
      "loss 0.117401406\n",
      "loss 0.11732973\n",
      "loss 0.117258176\n",
      "loss 0.117186666\n",
      "loss 0.117115095\n",
      "loss 0.11704366\n",
      "loss 0.11697222\n",
      "loss 0.116900824\n",
      "loss 0.116829515\n",
      "loss 0.11675825\n",
      "loss 0.116687\n",
      "loss 0.11661581\n",
      "loss 0.11654465\n",
      "loss 0.116473556\n",
      "loss 0.11640246\n",
      "loss 0.11633144\n",
      "loss 0.11626047\n",
      "loss 0.116189554\n",
      "loss 0.11611865\n",
      "loss 0.116047814\n",
      "loss 0.115976974\n",
      "loss 0.11590624\n",
      "loss 0.11583553\n",
      "loss 0.11576481\n",
      "loss 0.11569424\n",
      "loss 0.11562364\n",
      "loss 0.115553096\n",
      "loss 0.11548258\n",
      "loss 0.1154121\n",
      "loss 0.11534165\n",
      "loss 0.115271345\n",
      "loss 0.11520098\n",
      "loss 0.11513065\n",
      "loss 0.11506045\n",
      "loss 0.11499026\n",
      "loss 0.114920095\n",
      "loss 0.11484991\n",
      "loss 0.11477987\n",
      "loss 0.11470982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.11463986\n",
      "loss 0.11456992\n",
      "loss 0.1145\n",
      "loss 0.114430115\n",
      "loss 0.114360355\n",
      "loss 0.114290535\n",
      "loss 0.11422081\n",
      "loss 0.114151105\n",
      "loss 0.11408142\n",
      "loss 0.11401181\n",
      "loss 0.11394229\n",
      "loss 0.11387271\n",
      "loss 0.11380328\n",
      "loss 0.113733865\n",
      "loss 0.113664486\n",
      "loss 0.11359513\n",
      "loss 0.11352581\n",
      "loss 0.113456525\n",
      "loss 0.1133873\n",
      "loss 0.11331811\n",
      "loss 0.11324899\n",
      "loss 0.1131799\n",
      "loss 0.113110825\n",
      "loss 0.11304178\n",
      "loss 0.11297284\n",
      "loss 0.11290394\n",
      "loss 0.11283501\n",
      "loss 0.11276619\n",
      "loss 0.11269735\n",
      "loss 0.11262862\n",
      "loss 0.11255995\n",
      "loss 0.112491235\n",
      "loss 0.11242257\n",
      "loss 0.112353995\n",
      "loss 0.11228547\n",
      "loss 0.112216994\n",
      "loss 0.112148486\n",
      "loss 0.11208002\n",
      "loss 0.11201165\n",
      "loss 0.11194332\n",
      "loss 0.11187502\n",
      "loss 0.11180675\n",
      "loss 0.11173852\n",
      "loss 0.111670375\n",
      "loss 0.11160223\n",
      "loss 0.111534156\n",
      "loss 0.111466095\n",
      "loss 0.111398086\n",
      "loss 0.11133013\n",
      "loss 0.11126218\n",
      "loss 0.111194275\n",
      "loss 0.111126445\n",
      "loss 0.11105867\n",
      "loss 0.110990904\n",
      "loss 0.11092315\n",
      "loss 0.110855505\n",
      "loss 0.110787846\n",
      "loss 0.110720254\n",
      "loss 0.1106527\n",
      "loss 0.11058517\n",
      "loss 0.11051772\n",
      "loss 0.11045027\n",
      "loss 0.110382855\n",
      "loss 0.1103155\n",
      "loss 0.11024822\n",
      "loss 0.11018099\n",
      "loss 0.110113755\n",
      "loss 0.110046566\n",
      "loss 0.10997943\n",
      "loss 0.10991232\n",
      "loss 0.10984526\n",
      "loss 0.109778225\n",
      "loss 0.109711245\n",
      "loss 0.10964435\n",
      "loss 0.10957742\n",
      "loss 0.10951056\n",
      "loss 0.109443724\n",
      "loss 0.109377004\n",
      "loss 0.10931023\n",
      "loss 0.10924351\n",
      "loss 0.1091769\n",
      "loss 0.10911025\n",
      "loss 0.1090437\n",
      "loss 0.10897716\n",
      "loss 0.108910665\n",
      "loss 0.1088442\n",
      "loss 0.10877776\n",
      "loss 0.10871144\n",
      "loss 0.10864514\n",
      "loss 0.108578816\n",
      "loss 0.10851258\n",
      "loss 0.108446345\n",
      "loss 0.108380154\n",
      "loss 0.10831404\n",
      "loss 0.10824796\n",
      "loss 0.10818193\n",
      "loss 0.1081159\n",
      "loss 0.10804997\n",
      "loss 0.10798403\n",
      "loss 0.10791814\n",
      "loss 0.1078523\n",
      "loss 0.10778648\n",
      "loss 0.10772073\n",
      "loss 0.10765503\n",
      "loss 0.10758935\n",
      "loss 0.107523695\n",
      "loss 0.10745809\n",
      "loss 0.10739251\n",
      "loss 0.10732703\n",
      "loss 0.10726155\n",
      "loss 0.107196055\n",
      "loss 0.107130684\n",
      "loss 0.10706532\n",
      "loss 0.10699998\n",
      "loss 0.106934726\n",
      "loss 0.106869474\n",
      "loss 0.10680427\n",
      "loss 0.10673908\n",
      "loss 0.10667394\n",
      "loss 0.10660885\n",
      "loss 0.1065438\n",
      "loss 0.106478795\n",
      "loss 0.10641382\n",
      "loss 0.10634893\n",
      "loss 0.106284045\n",
      "loss 0.10621919\n",
      "loss 0.1061544\n",
      "loss 0.10608959\n",
      "loss 0.106024824\n",
      "loss 0.105960175\n",
      "loss 0.10589554\n",
      "loss 0.10583089\n",
      "loss 0.10576638\n",
      "loss 0.10570183\n",
      "loss 0.1056373\n",
      "loss 0.10557292\n",
      "loss 0.105508424\n",
      "loss 0.10544412\n",
      "loss 0.105379775\n",
      "loss 0.10531541\n",
      "loss 0.10525121\n",
      "loss 0.10518696\n",
      "loss 0.105122805\n",
      "loss 0.10505867\n",
      "loss 0.10499455\n",
      "loss 0.10493048\n",
      "loss 0.10486648\n",
      "loss 0.104802504\n",
      "loss 0.10473853\n",
      "loss 0.10467464\n",
      "loss 0.104610816\n",
      "loss 0.10454698\n",
      "loss 0.10448321\n",
      "loss 0.104419425\n",
      "loss 0.10435569\n",
      "loss 0.104292035\n",
      "loss 0.10422844\n",
      "loss 0.104164824\n",
      "loss 0.1041013\n",
      "loss 0.10403775\n",
      "loss 0.10397432\n",
      "loss 0.10391082\n",
      "loss 0.10384743\n",
      "loss 0.10378411\n",
      "loss 0.10372075\n",
      "loss 0.103657484\n",
      "loss 0.1035942\n",
      "loss 0.10353105\n",
      "loss 0.10346782\n",
      "loss 0.103404716\n",
      "loss 0.1033416\n",
      "loss 0.10327859\n",
      "loss 0.10321556\n",
      "loss 0.10315257\n",
      "loss 0.103089646\n",
      "loss 0.103026725\n",
      "loss 0.1029639\n",
      "loss 0.10290104\n",
      "loss 0.10283829\n",
      "loss 0.10277556\n",
      "loss 0.102712855\n",
      "loss 0.10265013\n",
      "loss 0.10258752\n",
      "loss 0.10252496\n",
      "loss 0.10246243\n",
      "loss 0.10239986\n",
      "loss 0.102337405\n",
      "loss 0.102274954\n",
      "loss 0.1022125\n",
      "loss 0.10215014\n",
      "loss 0.102087915\n",
      "loss 0.102025524\n",
      "loss 0.10196332\n",
      "loss 0.10190113\n",
      "loss 0.10183895\n",
      "loss 0.1017768\n",
      "loss 0.10171467\n",
      "loss 0.10165266\n",
      "loss 0.10159063\n",
      "loss 0.10152863\n",
      "loss 0.10146673\n",
      "loss 0.10140486\n",
      "loss 0.10134296\n",
      "loss 0.101281084\n",
      "loss 0.10121928\n",
      "loss 0.10115752\n",
      "loss 0.101095855\n",
      "loss 0.10103414\n",
      "loss 0.10097249\n",
      "loss 0.10091095\n",
      "loss 0.100849316\n",
      "loss 0.10078776\n",
      "loss 0.10072631\n",
      "loss 0.10066484\n",
      "loss 0.10060339\n",
      "loss 0.100542024\n",
      "loss 0.10048071\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=multi_regression) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        _, l = sess.run([train,cost],feed_dict = {X:multi_x, Y:multi_y})\n",
    "        print('loss',l)\n",
    "    W_multi, b_multi = sess.run([W,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.5537517666816711\n",
      "2.0 2.28256756067276\n",
      "3.0 3.158804953098297\n",
      "4.0 3.813909947872162\n",
      "5.0 4.763858139514923\n",
      "[[0.8025266 0.7656712]] 0.7512252\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(multi_x)):\n",
    "    sum = 0\n",
    "    for i in range(2):\n",
    "        sum += W_multi[0][i] * multi_x[j,i]\n",
    "    print(multi_y[j], sum+b_multi)\n",
    "print(W_multi,b_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = np.loadtxt('logistic_regression.txt')\n",
    "logistic_x = logistic[:,:-1]\n",
    "logistic_y = logistic[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as logistic_regression:\n",
    "    num_x = len(logistic_x[0])\n",
    "    X = tf.placeholder(tf.float32, [None, num_x], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None], name='Y')\n",
    "    lr = tf.constant(1e-1,tf.float32)\n",
    "    W = tf.get_variable('W', [1,num_x], tf.float32)\n",
    "    b = tf.get_variable('b',dtype=tf.float32,initializer=tf.constant(1.,tf.float32))\n",
    "    \n",
    "    h = tf.matmul(W,X,transpose_b=True) + b\n",
    "    hypothesis=tf.sigmoid(h)\n",
    "    cost = -tf.reduce_mean((1-Y) * tf.log(hypothesis)+Y*tf.log(1-hypothesis)) #분류형 모형일 때 쓰는 교차함수를 풀어서 쓴 것\n",
    "    #Y가 0일 땐 tf.log(hypothesis)값이 최소가 되도록(0에 가까워지도록), Y가 1일땐 tf.log(1-hypothesis)값이 최소가 되도록 하면\n",
    "    #cost값이 최소가 됨 #불순도 파악하던 엔트로피 그래프 모양 생각\n",
    "    train = tf.train.GradientDescentOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
